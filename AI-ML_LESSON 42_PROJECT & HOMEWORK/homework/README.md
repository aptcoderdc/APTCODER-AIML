Mini-batch Gradient Descent Algorithm Implementation

## Overview
This homework demonstrates the implementation of the mini-batch gradient descent algorithm to optimize parameters for a simple linear regression model.

## Concepts Covered
Mini-batch Gradient Descent
Linear Regression
Mean Squared Error (MSE) Loss Function

## Required Libraries
numpy

## Setup
Clone the project repository to your local machine.
Make sure you have Python installed.
Install the required libraries by running: pip install numpy.

## How to Run
Open the mini_batch_gradient_descent.py file in your preferred Python IDE or text editor.
Run the script.
The optimized slope (m) and intercept (c) parameters will be printed to the console.

## Project Structure
mini_batch_gradient_descent.py: Python script containing the implementation of the mini-batch gradient descent algorithm.
README.md: Overview, setup instructions, and usage guidelines for the homework.

## Additional Notes
This homework uses dummy data for demonstration purposes.
Adjust the parameters batch_size, learning_rate, and epochs in the mini_batch_gradient_descent function to observe different optimization results.
Feel free to modify and experiment with the code to gain a better understanding of mini-batch gradient descent.